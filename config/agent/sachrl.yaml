# Defaults for SACHRL agent, with tuned parameters from D2RL: Deep Dense
# Architectures in Reinforcement Learning

name: 'sachrl'
gamma: 0.99  # discount factor
polyak: 0.995  # polyak step size for updating target network
batch_size: 256
rpbuf_size: 1e6  # replay buffer size
rpbuf_device: auto
samples_per_update: 50  # perform updates every N samples
num_updates: ${agent.samples_per_update}
warmup_samples: 1000
randexp_samples: 1000
clip_grad_norm: 0.0 # only used if > 0
dyne_updates: true

alpha: 0.1
target_entropy_factor: 1.0
optim_alpha:
  _target_: torch.optim.Adam
  lr: 1e-4

action_interval: 10
action_interval_anneal: 0  # anneal towards native actions over this many frames
action_interval_min: 2

policy_lo:
  cond_key: z
  cond_dim: 32
  obs_key: observation

upsampler:
  type: null # null, matrix, model
  path: null
  # model definition can go here

record_upsampled_actions: false

tanh_actions: true
action_cost: 0.0
action_cost_type: square
action_factor_hi: 1
hide_from_lo: null # hide these features from the low-level observation
hide_obs_lo_from_hi: false # hide ${obs_lo} from high-level policy

graph: true
