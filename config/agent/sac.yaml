# Defaults for SAC agent, with tuned parameters from D2RL: Deep Dense
# Architectures in Reinforcement Learning

name: 'sac'
gamma: 0.99  # discount factor
polyak: 0.995  # polyak step size for updating target network
batch_size: 256
rpbuf_size: 1e6  # replay buffer size
rpbuf_device: auto
samples_per_update: 50  # perform updates every N samples
num_updates: ${agent.samples_per_update}
policy_delay: 1  # 1 = no delay
policy_target: min
warmup_samples: 1000  # use first N samples to fill replay buffer
randexp_samples: 1000  # explore with random actions for first N samples
flatten_obs: true
clip_grad_norm: 0.0 # only used if > 0

alpha: 0.1  # If optim_alpha is not null, this is just the initial alpha value
target_entropy_factor: 1.0 # If optim_alpha is not null
optim_alpha:  # Can be set to null to disable entropy coefficient learning and use target_entropy instead.
  _target_: torch.optim.Adam
  lr: 1e-4

graph: true
